ğŸ“„ Conversational RAG with PDFs (Streamlit + LangChain)

This project implements a Conversational Retrieval-Augmented Generation (RAG) application using Streamlit, LangChain, Groq LLMs, Chroma vector store, and HuggingFace embeddings.
Users can upload one or more PDFs and ask questions conversationally, with chat history awareness across sessions.

ğŸš€ Features

ğŸ“¤ Upload multiple PDF files

ğŸ” Semantic search using embeddings

ğŸ§  Context-aware question reformulation using chat history

ğŸ’¬ Conversational Q&A with memory (session-based)

âš¡ Fast inference using Groq (Gemma2-9B-IT)

ğŸ—‚ï¸ Vector storage using Chroma

ğŸ§µ Persistent chat history per session

ğŸ—ï¸ Architecture Overview
User Question
     â”‚
     â–¼
Chat History â†’ History-Aware Retriever
     â”‚
     â–¼
Relevant PDF Chunks (Chroma)
     â”‚
     â–¼
LLM (Groq Gemma2-9B-IT)
     â”‚
     â–¼
Concise Answer (with memory)

ğŸ§° Tech Stack

Streamlit â€“ UI

LangChain â€“ RAG pipeline

Groq â€“ LLM inference

HuggingFace Embeddings â€“ all-MiniLM-L6-v2

ChromaDB â€“ Vector store

PyPDFLoader â€“ PDF parsing

ğŸ“¦ Installation
1ï¸âƒ£ Clone the Repository
git clone https://github.com/your-username/conversational-rag-pdf.git
cd conversational-rag-pdf

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv
source venv/bin/activate   # Mac/Linux
venv\Scripts\activate      # Windows

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

ğŸ” Environment Variables

Create a .env file in the root directory:

HF_TOKEN=your_huggingface_token


ğŸ’¡ The Groq API key is entered directly in the Streamlit UI for security.

â–¶ï¸ Run the Application
streamlit run app.py

ğŸ–¥ï¸ How to Use

Enter your Groq API Key

Provide a Session ID (used for chat memory)

Upload one or more PDF files

Ask questions related to the PDFs

Continue chatting with contextual memory ğŸ¯

ğŸ§  How Chat History Works

Each session ID maintains its own ChatMessageHistory

Questions are reformulated using previous messages

Enables follow-up questions like:

â€œExplain that in simpler termsâ€
â€œWhat was the conclusion?â€

âš™ï¸ Key Components Explained
ğŸ”¹ History-Aware Retriever

Rewrites follow-up questions into standalone queries using past conversation context.

ğŸ”¹ Retrieval Chain

Fetches relevant document chunks from Chroma and injects them into the LLM prompt.

ğŸ”¹ RunnableWithMessageHistory

Maintains stateful conversations across user inputs.

ğŸ“Œ Limitations

Temporary PDF storage (temp.pdf) is overwritten per upload

Chroma vector store is in-memory (not persistent)

Designed for small to medium PDFs

ğŸ› ï¸ Possible Enhancements

âœ… Persistent vector storage

âœ… File-specific metadata filtering

âœ… Streaming responses

âœ… Multi-user authentication

âœ… Source citation in answers

ğŸ“œ License

MIT License

ğŸ™Œ Acknowledgements

LangChain

Groq

HuggingFace

Streamlit
